{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN dimensions stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os,re,csv,pathlib\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c1 = nn.Conv2d(1, 8, kernel_size=(15,2))\n",
    "m1 = nn.MaxPool2d(kernel_size = (1,2))\n",
    "c2 = nn.Conv2d(8, 16, kernel_size = (1,2))\n",
    "m2 = nn.MaxPool2d(kernel_size = (1,2))\n",
    "\n",
    "cnn = nn.Sequential(c1,m1,c2,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "\n",
      "USING LENGTH =  12\n",
      "Input:  torch.Size([1, 1, 15, 12])\n",
      "After Conv1: torch.Size([1, 8, 1, 11])\n",
      "After Pool1: torch.Size([1, 8, 1, 5])\n",
      "After Conv2: torch.Size([1, 16, 1, 4])\n",
      "After Pool2: torch.Size([1, 16, 1, 2])\n",
      "Dense torch.Size([1, 10])\n",
      "soft tensor([[0.0929, 0.1150, 0.0869, 0.1024, 0.0870, 0.0853, 0.1370, 0.1060, 0.0725,\n",
      "         0.1151]], grad_fn=<SoftmaxBackward>)\n",
      "arg tensor([6])\n",
      "####\n",
      "\n",
      "\n",
      "####\n",
      "\n",
      "USING LENGTH =  13\n",
      "Input:  torch.Size([1, 1, 15, 13])\n",
      "After Conv1: torch.Size([1, 8, 1, 12])\n",
      "After Pool1: torch.Size([1, 8, 1, 6])\n",
      "After Conv2: torch.Size([1, 16, 1, 5])\n",
      "After Pool2: torch.Size([1, 16, 1, 2])\n",
      "Dense torch.Size([1, 10])\n",
      "soft tensor([[0.0969, 0.1168, 0.0839, 0.0816, 0.1263, 0.1539, 0.1043, 0.1046, 0.0517,\n",
      "         0.0800]], grad_fn=<SoftmaxBackward>)\n",
      "arg tensor([5])\n",
      "####\n",
      "\n",
      "\n",
      "####\n",
      "\n",
      "USING LENGTH =  14\n",
      "Input:  torch.Size([1, 1, 15, 14])\n",
      "After Conv1: torch.Size([1, 8, 1, 13])\n",
      "After Pool1: torch.Size([1, 8, 1, 6])\n",
      "After Conv2: torch.Size([1, 16, 1, 5])\n",
      "After Pool2: torch.Size([1, 16, 1, 2])\n",
      "Dense torch.Size([1, 10])\n",
      "soft tensor([[0.0755, 0.1010, 0.1032, 0.0996, 0.1109, 0.0994, 0.1139, 0.0760, 0.1319,\n",
      "         0.0887]], grad_fn=<SoftmaxBackward>)\n",
      "arg tensor([8])\n",
      "####\n",
      "\n",
      "\n",
      "####\n",
      "\n",
      "USING LENGTH =  15\n",
      "Input:  torch.Size([1, 1, 15, 15])\n",
      "After Conv1: torch.Size([1, 8, 1, 14])\n",
      "After Pool1: torch.Size([1, 8, 1, 7])\n",
      "After Conv2: torch.Size([1, 16, 1, 6])\n",
      "After Pool2: torch.Size([1, 16, 1, 3])\n",
      "Dense torch.Size([1, 10])\n",
      "soft tensor([[0.1084, 0.0856, 0.0899, 0.0999, 0.0959, 0.0812, 0.1071, 0.1197, 0.1192,\n",
      "         0.0932]], grad_fn=<SoftmaxBackward>)\n",
      "arg tensor([7])\n",
      "####\n",
      "\n",
      "\n",
      "####\n",
      "\n",
      "USING LENGTH =  16\n",
      "Input:  torch.Size([1, 1, 15, 16])\n",
      "After Conv1: torch.Size([1, 8, 1, 15])\n",
      "After Pool1: torch.Size([1, 8, 1, 7])\n",
      "After Conv2: torch.Size([1, 16, 1, 6])\n",
      "After Pool2: torch.Size([1, 16, 1, 3])\n",
      "Dense torch.Size([1, 10])\n",
      "soft tensor([[0.0902, 0.1176, 0.0941, 0.1054, 0.0936, 0.0894, 0.1269, 0.0940, 0.1138,\n",
      "         0.0751]], grad_fn=<SoftmaxBackward>)\n",
      "arg tensor([6])\n",
      "####\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in range(12,17):\n",
    "    print(\"####\\n\\nUSING LENGTH = \",z)\n",
    "    x = torch.rand((1,1,15,z),dtype=torch.float32)\n",
    "    print(\"Input: \",x.shape)\n",
    "    conv1=c1(x)\n",
    "    print(\"After Conv1:\", conv1.shape)\n",
    "    pool1=m1(conv1)\n",
    "    print(\"After Pool1:\", pool1.shape)\n",
    "    conv2=c2(pool1)\n",
    "    print(\"After Conv2:\", conv2.shape)\n",
    "    pool2=m2(conv2)\n",
    "    print(\"After Pool2:\", pool2.shape)\n",
    "    if z<15:\n",
    "        xx = 16*2\n",
    "    elif z>=15:\n",
    "        xx = 16*3\n",
    "    lin = nn.Linear(xx,10)\n",
    "    \n",
    "    dense1= lin(pool2.view(-1,xx))\n",
    "    print(\"Dense\",dense1.shape)\n",
    "    print(\"soft\",dense1.softmax(1))\n",
    "    print(\"arg\",dense1.argmax(1))\n",
    "    print(\"####\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing/AA index stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\t-8.36918251369427\t8.3031934498954\t-6.6196694564762\t13.8734139891373\t8.59531323791867\t9.79914818190207\t1.26911186662177\t-4.63752901352054\t-0.983921565990354\t4.3634243590938\t2.34365256090458\t0.343445840449375\t1.18290876288827\t0.0132558803925318\t-0.696621502763574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAs=np.array(list('WFGAVILMPYSTNQCKRHDE'))\n",
    "curPath=os.getcwd()\n",
    "##AAidx_file='AAindexNormalized.txt' ## AA index reached AUC about 61% for L=14. Worse than AdaBoost\n",
    "##AAidx_file='AtchleyFactors.txt'  ## Atchley factors work worse than using 544 AA index\n",
    "AAidx_file='AAidx_PCA.txt' ## works like a charm!!!\n",
    "gg=open(AAidx_file)\n",
    "AAidx_Names=gg.readline().strip().split('\\t') # Get PC1,... PC15\n",
    "AAidx_Dict={} # Gets the values for each of the 15 features\n",
    "i = 0\n",
    "for ll in gg.readlines():\n",
    "    if i ==4:\n",
    "        print(ll)\n",
    "    \n",
    "    ll=ll.strip().split('\\t')\n",
    "    AA=ll[0]\n",
    "    tag=0\n",
    "    vv=[]\n",
    "    for xx in ll[1:]:\n",
    "        vv.append(float(xx))\n",
    "    if tag==1:\n",
    "        i+=1\n",
    "        continue\n",
    "    i+=1\n",
    "    AAidx_Dict[AA]=vv\n",
    "    \n",
    "Nf=len(AAidx_Dict['C']) #could've just written 15...\n",
    "\n",
    "pat=re.compile('[\\\\*_XB]')  ## non-productive CDR3 patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Save the output of the mapping AAidx_dict in the beginning of DeepCAT.py\n",
    "with open('AAidx_dict.pkl', 'wb') as f: \n",
    "    pickle.dump(AAidx_Dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 \n",
      " (20, 15) \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "AAs=np.array(list('WFGAVILMPYSTNQCKRHDE'))\n",
    "\n",
    "def OneHotEncoding(Seq):\n",
    "    Seq_aa = list(Seq) \n",
    "    Ns=len(Seq_aa)\n",
    "    OHE=np.zeros([20,Ns])\n",
    "    for ii in range(Ns):\n",
    "        aa=Seq_aa[ii]\n",
    "        vv=np.where(AAs==aa)\n",
    "        OHE[vv,ii]=1\n",
    "    OHE=OHE.astype(np.float32)\n",
    "    return OHE\n",
    "#So in X (row/index) we have the amino acid \n",
    "#And in Y (columns) we have the position in the sequence\n",
    "seq = 'CASSYSTRGGSPLHF'\n",
    "temp = OneHotEncoding(seq)\n",
    "print(temp.dtype,\"\\n\", temp.shape,\"\\n\",temp,\"\\n\",type(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 \n",
      " (15, 15) \n",
      " [[-8.369e+00 -9.709e-01  1.185e+01  1.185e+01 -7.499e+00  1.185e+01\n",
      "   4.530e+00  8.538e+00  1.484e+01  1.484e+01  1.185e+01  1.622e+01\n",
      "  -1.764e+01  6.881e-01 -1.860e+01]\n",
      " [ 8.303e+00 -3.237e-01  6.885e+00  6.885e+00  1.310e+00  6.885e+00\n",
      "   5.127e+00 -1.379e+01  1.924e+01  1.924e+01  6.885e+00  1.509e+01\n",
      "  -3.524e-01 -6.180e+00  9.224e-01]\n",
      " [-6.620e+00  1.572e+01  2.967e+00  2.967e+00 -1.208e+01  2.967e+00\n",
      "   1.436e+00 -4.771e+00  5.934e+00  5.934e+00  2.967e+00 -8.723e+00\n",
      "   1.184e+01 -6.806e+00 -3.317e+00]\n",
      " [ 1.387e+01 -5.088e-01  3.629e+00  3.629e+00 -1.161e+00  3.629e+00\n",
      "   1.768e+00 -1.164e+00  5.667e+00  5.667e+00  3.629e+00 -1.878e+01\n",
      "  -5.375e+00  3.947e+00 -2.522e+00]\n",
      " [ 8.595e+00  3.740e+00 -2.886e+00 -2.886e+00 -6.895e+00 -2.886e+00\n",
      "  -3.393e+00 -9.466e+00 -5.819e+00 -5.819e+00 -2.886e+00  6.465e+00\n",
      "  -1.160e+00  1.333e+00 -4.513e-01]\n",
      " [ 9.799e+00 -7.788e-01  2.324e+00  2.324e+00 -3.231e+00  2.324e+00\n",
      "   5.244e+00  6.792e+00 -8.789e+00 -8.789e+00  2.324e+00  4.562e+00\n",
      "  -1.972e+00 -5.121e-01 -4.092e+00]\n",
      " [ 1.269e+00  3.387e+00 -1.338e+00 -1.338e+00 -4.862e+00 -1.338e+00\n",
      "  -3.364e+00  1.990e+00  5.739e+00  5.739e+00 -1.338e+00  3.056e+00\n",
      "   8.551e-01  4.738e+00 -3.993e-01]\n",
      " [-4.638e+00 -9.131e-01  3.317e+00  3.317e+00  6.882e-01  3.317e+00\n",
      "   2.201e+00 -5.153e+00 -4.728e+00 -4.728e+00  3.317e+00  1.569e-01\n",
      "   5.306e-01  8.747e+00  2.411e+00]\n",
      " [-9.839e-01  3.006e+00  6.264e+00  6.264e+00 -2.455e+00  6.264e+00\n",
      "   6.527e+00 -2.938e+00 -4.203e+00 -4.203e+00  6.264e+00 -6.693e-01\n",
      "   2.120e+00 -3.124e+00 -1.084e+00]\n",
      " [ 4.363e+00 -2.329e+00 -8.979e-01 -8.979e-01  1.482e+00 -8.979e-01\n",
      "  -4.746e+00 -5.892e+00 -1.565e+00 -1.565e+00 -8.979e-01  7.339e-01\n",
      "   4.052e+00 -1.576e+00 -1.471e+00]\n",
      " [ 2.344e+00  7.870e-01  7.394e-01  7.394e-01 -1.429e+00  7.394e-01\n",
      "  -2.040e+00  5.270e+00 -1.375e+00 -1.375e+00  7.394e-01  3.953e-01\n",
      "   7.067e+00  1.990e+00  3.194e+00]\n",
      " [ 3.434e-01 -1.874e+00  1.150e+00  1.150e+00  2.182e+00  1.150e+00\n",
      "  -6.506e-01  1.610e+00 -6.125e-01 -6.125e-01  1.150e+00 -1.651e-01\n",
      "   2.575e+00 -7.789e+00  2.044e+00]\n",
      " [ 1.183e+00  1.533e+00  1.361e+00  1.361e+00  7.447e+00  1.361e+00\n",
      "   1.927e-01 -1.952e+00  6.617e-01  6.617e-01  1.361e+00 -4.381e-01\n",
      "   1.455e+00  9.985e-01  2.944e-01]\n",
      " [ 1.326e-02  1.344e+00  1.133e+00  1.133e+00  3.078e+00  1.133e+00\n",
      "  -2.544e-01 -6.441e-01 -6.992e-01 -6.992e-01  1.133e+00  5.203e-02\n",
      "   1.283e+00  1.376e+00 -3.188e+00]\n",
      " [-6.966e-01  3.358e+00  1.164e+00  1.164e+00  2.849e+00  1.164e+00\n",
      "  -3.256e+00  1.539e+00 -1.406e+00 -1.406e+00  1.164e+00  5.056e-01\n",
      "  -2.655e-01 -4.312e-01 -6.456e+00]] \n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Nf = number features = 15\n",
    "def AAindexEncoding(Seq):\n",
    "    Ns=len(Seq)\n",
    "    AAE=np.zeros([Ns, Nf])\n",
    "    for kk in range(Ns):\n",
    "        ss=Seq[kk]\n",
    "        AAE[kk,]=AAidx_Dict[ss]\n",
    "    AAE=np.transpose(AAE.astype(np.float32))\n",
    "    return AAE\n",
    "seq = 'CASSYSTRGGSPLHF'\n",
    "temp = AAindexEncoding(seq)\n",
    "print(temp.dtype,\"\\n\", temp.shape,\"\\n\",temp,\"\\n\",type(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFeatureLabels(TumorCDR3s, NonTumorCDR3s):\n",
    "    nt=len(TumorCDR3s)\n",
    "    nc=len(NonTumorCDR3s)\n",
    "    LLt=[len(ss) for ss in TumorCDR3s]\n",
    "    LLt=np.array(LLt)\n",
    "    LLc=[len(ss) for ss in NonTumorCDR3s]\n",
    "    LLc=np.array(LLc)\n",
    "    NL=range(12,17)\n",
    "    FeatureDict={}\n",
    "    LabelDict={}\n",
    "    for LL in NL:\n",
    "        vvt=np.where(LLt==LL)[0]\n",
    "        vvc=np.where(LLc==LL)[0]\n",
    "        Labels=[1]*len(vvt)+[0]*len(vvc)\n",
    "        Labels=np.array(Labels)\n",
    "        Labels=Labels.astype(np.int32)\n",
    "        data=[]\n",
    "        for ss in TumorCDR3s[vvt]:\n",
    "            if len(pat.findall(ss))>0:\n",
    "                continue\n",
    "            data.append(AAindexEncoding(ss))\n",
    "#            data.append(OneHotEncoding(ss))\n",
    "        for ss in NonTumorCDR3s[vvc]:\n",
    "            if len(pat.findall(ss))>0:\n",
    "                continue\n",
    "            data.append(AAindexEncoding(ss))\n",
    "#            data.append(OneHotEncoding(ss))\n",
    "        data=np.array(data)\n",
    "        features={'x':data,'LL':LL}\n",
    "        FeatureDict[LL]=features\n",
    "        LabelDict[LL]=Labels\n",
    "    return FeatureDict, LabelDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
